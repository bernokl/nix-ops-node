* This is document to capture standing up nodes in cardano using divnix and terraform
- This document belongs with https://github.com/bernokl/nix-ops-node
- We are going to explore standing up cardano nodes using out existing divnix and terraform pattern
- The end goal is autonomous deploy of iohk/cardano-node flakes to have relay and then blockproducers.

** Stand up relay node
- Here is the steps I aim to follow:
#+begin_example
- Deploy nixos ec2 instance in aws using terraform
- Clone the cardano-node repository.
- In the cardano-node repository, create a new file called configuration.nix.
- In the configuration.nix file, add the following code:
       {
         imports = [
           "github:input-output-hk/cardano-node?ref=master"
         ];
       }
- Run the following command to build the cardano node:
       nix build github:input-output-hk/cardano-node?ref=master
- Once the cardano node is built, you can start it by running the following command:
       nix run github:input-output-hk/cardano-node?ref=master run
- Here are some additional details about the instructions above:
  - The imports section of the configuration.nix file specifies the Nix flakes that the cardano node depends on.
  - The nix build command builds the cardano node from the Nix flakes that are specified in the imports section.
  - The nix run command starts the cardano node.
  - The cardano-cli tool is used to interact with the cardano node.
#+end_example
- In my repo I am going to copy the terrafor/cache-server to make my relay-node folder.
- I am going to strip the terraform to give me just aws instance:
- I enable envrc with:
#+begin_src tmux :session s1
direnv allow .
#+end_src
- That loads my aws keys into env
- TODO: Ongoing reminder that we need to think about credentials.
- Run init
#+begin_src tmux :session s1
aws_terraform_init
#+end_src
- Apply:
#+begin_src tmux :session s1
aws_terraform_apply
#+end_src
- Grab ip from aws-console ssh in
#+begin_src tmux :session s1
ssh -i id_rsa.pem root@xx.xx.xx.xx
#+end_src
- OK, lets run:
#+begin_src tmux :session s1
nix build github:input-output-hk/cardano-node?ref=master
#+end_src
- Build was less than 3 minuts lets try the run
- Lets pass in run
#+begin_src tmux :session s1
nix run github:input-output-hk/cardano-node?ref=master run
#+end_src
- New error! OO this is why they want you to clone the rope first, lets go look
#+begin_example
InvalidYaml (Just (YamlException "Yaml file not found: configuration/cardano/mainnet-config.json"))

cardano-node: YAML exception:
Yaml file not found: configuration/cardano/mainnet-config.json
#+end_example
- Clone repo to our server
#+begin_src tmux :session s1
git clone https://github.com/input-output-hk/cardano-node.git
#+end_src
- cd
#+begin_src tmux :session s1
  cd cardano-node
#+end_src
- Lets create the configuration.nix
#+begin_example
{
  imports = [
    "github:input-output-hk/cardano-node?ref=master"
  ];
}

#+end_example
- Run from inside repo
#+begin_src tmux :session s1
nix run github:input-output-hk/cardano-node?ref=master run
#+end_src
- Boom, we have a running relay.
- The above should be very easy to add to user_data in terraform.
- Lets strip user_data out to file and:
   - Clone repo
   - Add our configuration.nix
   - Build
   - Run
- Lets update the main.tf to have this:
#+begin_example
 user_data = "${file("start_node.sh")}"
#+end_example
- And lets go crate a start_node.sh
#+begin_example
#!env bash -xe
git clone https://github.com/input-output-hk/cardano-node.git &&
cd cardano-node
cat << 'EOF' > configuration.nix
{
  imports = [
    "github:input-output-hk/cardano-node?ref=master"
  ];
}
EOF
yes | nix build github:input-output-hk/cardano-node?ref=master && 
echo node_done_building > /tmp/outNix
yes | nix run github:input-output-hk/cardano-node?ref=master run
#+end_example
- Now we destroy the host and start the apply again so that start_node.sh can run.
## current:
- We can manually build and start a cardano relay by running: aws_terraform_apply
- Right now the node is up, and I see a  process: nix build github:input-output-hk/cardano-node?ref=master
- If I strace there is activity, also we are steadily using more disk space.
- I do not understand why my manual build was so much quicker. 
- It has been building for almost exactly 2 hours. I do see the load is 12 on 4 cores meaning the cpu is not nearly keeping up. 
- I think I might have scaled to 2xlarge or even 4xlarge for the build phase in diypool, might have to consider doing the same here.
- Will leave it to run for now, I wish I had a sense of % done
- It took a couple of hours of google and play, but came up with:
#+begin_src tmux :session s1
nix build --accept-flake-config github:input-output-hk/cardano-node?ref=master
#+end_src
- I got it to work, but still have some unexpected behaviuor.
- Let me destroy and rebuild with everything vanilla then try to run nix build --accept-flake-config on clean os
- YAS, running this manually builds a node I can run in 5 minutes
#+begin_src tmux :session s1
nix build --accept-flake-config github:input-output-hk/cardano-node?ref=master
#+end_src
- OK going to destroy and build with that in my startup_node.sh
#+begin_example
nix build --accept-flake-config github:input-output-hk/cardano-node?ref=master &&
echo we_got_clean_build > /tmp/outNix
nix run --accept-flake-config github:input-output-hk/cardano-node?ref=master run
#+end_example
- This builds the node! and I think starts it, BUT it does not say running.
- The matrue solution would be to have the run executed by a daemon service like system.d.
- At this point I fell into a multi hour investigation into adding traditional /etc/systemd/system/*.service I could run.
- Turns out nix wants you to define your service in configurattion.nix
- It seems like you add it to configurion something like:
#+begin_example
config.systemd.services.interosEsMdb = {
  description = "Interos MongoDB+ES log capture";
  after = ["network.target"];
  wantedBy = ["multi-user.target"];

  serviceConfig = {
    # change this to refer to your actual derivation
    ExecStart = "${interosEsMdb}/bin/syslog-exec.sh";
    EnvironmentFile = "${interosEsMdb}/lib/es-service.env";
    Restart = "always";
    RestartSec = 1;
  }
#+end_example
- Lots of itteration later I ended with this:
#+begin_example
  systemd.services.cardano-node-relay-daemon = {
    enable = true;
    description = "Cardano relay daemon";
    after = ["network.target"];
    wantedBy = ["multi-user.target"];

    serviceConfig = {
      ExecStart = "${pkgs.nix}/bin/nix run --accept-flake-config github:input-output-hk/cardano-node?ref=master run";
      Restart = "always";
      User = "root";
      WorkingDirectory="/cardano-node/";
      RestartSec = 1;
    };
  };
#+end_example
- I also needed to add line to startup_node.sh to start the service
#+begin_example
systemctl start cardano-node-relay-daemon.service
#+end_example

- Nice cheat to find aws ec2 external ip, replace running with tag or other metadata you car about
#+begin_src tmux :session s1
aws ec2 describe-instances --filters 'Name=instance-state-name,Values=running' --query 'Reservations[*].Instances[*].[InstanceId,PublicIpAddress]' --output text
#+end_src
- returns:
#+begin_example
i-01a7e8d4e89049894     13.239.136.44
#+end_example
- And on this host I see the daemon running:
#+begin_example
> systemctl status cardano-node-relay-daemon.service 
â— cardano-node-relay-daemon.service - Cardano relay daemon
     Loaded: loaded (]8;;file://ip-172-31-19-21.ap-southeast-2.compute.internal/etc/systemd/system/cardano-node-relay-daemon.service/etc/systemd/system/cardano-node-relay-daemon.service]8;;; enabled; preset: enabled)]8;;
     Active: active (running) since Mon 2023-05-08 13:47:51 UTC; 4h 24min ago
   Main PID: 2101 (cardano-node)
         IP: 15.2G in, 157.6M out
         IO: 316.0K read, 18.3G written
      Tasks: 16 (limit: 9155)
     Memory: 6.1G
        CPU: 8h 50min 11.556s
     CGroup: /system.slice/cardano-node-relay-daemon.service
             â””â”€2101 /nix/store/0ndig34c9qizj3g4z1s1scwk3pxcvfzn-cardano-node-exe-cardano-node-8.0.0/bin/cardano-node>

May 08 18:12:16 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:18 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:19 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:20 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:21 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:23 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:24 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:25 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:26 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:28 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
#+end_example
- Robert suggeted I focus on tailscale integration next as he already had the template on hokioi
- Here is the updates I made to get the ec2 instance running tailscale and connecting to yumi network
#+begin_example
 { config, lib, pkgs, modulesPath, ... }:
+let
+   system.autoUpgrade.channel = "https://nixos.org/channels/nixos-unstable";
+   nixos-unstable = import <nixos-unstable> {};
 
-{
+in {
   imports = [ "${modulesPath}/virtualisation/amazon-image.nix" ];
 
   ec2.hvm = true;
@@ -26,10 +29,47 @@
     };
   };
 
+ services.tailscale.enable = true;
+
+ systemd.services.tailscale-autoconnect = {
+    description = "Automatic connection to Tailscale";
+
+    # make sure tailscale is running before trying to connect to tailscale
+    after = [ "network-pre.target" "tailscale.service" ];
+    wants = [ "network-pre.target" "tailscale.service" ];
+    wantedBy = [ "multi-user.target" ];
+
+    # set this service as a oneshot job
+    serviceConfig.Type = "oneshot";
+
+    # have the job run this shell script
+    script = with pkgs; ''
+      # wait for tailscaled to settle
+      sleep 2
+
+      # check if we are already authenticated to tailscale
+      status="$(${tailscale}/bin/tailscale status -json | ${jq}/bin/jq -r .BackendState)"
+      if [ $status = "Running" ]; then # if so, then do nothing
+        exit 0
+      fi
+
+      # otherwise authenticate with tailscale
#
+      ${tailscale}/bin/tailscale up --ssh -authkey tskey-auth-########
+    '';
+};
+
+  networking.firewall = {
+    checkReversePath = "loose";
+    enable = true;
+    trustedInterfaces = [ "tailscale0" ];
+    allowedUDPPorts = [ config.services.tailscale.port ];
+  };
+
+  networking.hostName = "aws-1";
+  networking.domain = "husky-ostrich.ts.net";

   environment.systemPackages = with pkgs; [
     git
     vim
     htop
+    tailscale
     lsof
   ];
 }

#+end_example
- Note we give the machine a networking.hostName, that registers the name we want for this machine in tailscale
- Also VERY important once it is connected to tailscale your ssh sessions over the 10. network with be authenticated through tailscale.
- This is a very important bennefit.
- Also very important, the authkey used needs to be set to be ephemiral, pre-authenticate the hosts and assign tags we want for the machines.
- This makes management very simple, but needs to be carefully managed.
- We will integrate SOPS/1Password/Key-store to hold keys we can then hydrate on host with env-vars in our session.
- Trying to do some testing, lets start with what we can see on the node:
#+begin_example
journalctl -u cardano-node-relay-daemon.service

May 10 18:08:04 aws-1 nix[2100]: Event: LedgerUpdate (HardForkUpdateInEra S (S (Z (WrapLedgerUpdate {unwrapLedgerUpdate = ShelleyUpdatedProtocolUpdates []}))))
May 10 18:08:04 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:04.96 UTC] Chain extended, new tip: 2198c40091993baed54b4638473327b0b77c5dccaa56768690f5b56>
May 10 18:08:06 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:06.21 UTC] Chain extended, new tip: 2e7ccf635d45201aaf52c5a2e7e10f7c5b90a2ca5ed10356210859e>
May 10 18:08:07 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:07.46 UTC] Chain extended, new tip: 18ba572b54363a6bcb43bccb283828ab559cd5ddf7d73c2b0c07c51>
May 10 18:08:08 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:08.71 UTC] Chain extended, new tip: 87cfa4a2f2258217adbde872e2ab53906f43d95e1f9fbbbc4dc362a>
May 10 18:08:09 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:09.53 UTC] Chain extended, new tip: 8add136563b0c36616f42ad52815e3c666be1b8b087e4268949395c>
May 10 18:08:09 aws-1 nix[2100]: Event: LedgerUpdate (HardForkUpdateInEra S (S (Z (WrapLedgerUpdate {unwrapLedgerUpdate = ShelleyUpdatedProtocolUpdates [ProtocolUpdate {protocolUpda>
May 10 18:08:09 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:09.54 UTC] Chain extended, new tip: e6878f21c35b5c9c233bf54207c28dbeb0743c6cf19d1468afc78c6>
May 10 18:08:10 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:10.79 UTC] Chain extended, new tip: 92fc5c7b7e6b8a84623787b2b3a52400d9388958258e7cdf57c9d6f>
May 10 18:08:12 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:12.04 UTC] Chain extended, new tip: f02680481aa08d0d53b4d1574d063fcba04d8b10b8e91ef07c52737>
May 10 18:08:13 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:13.29 UTC] Chain extended, new tip: 83fdc9ce9b078a25880964ab7c87bf5e66a73849fbe06918580f738>
#+end_example
- That seems positive confirmation that we are participating in the network traffic.
- Going to try to query the node using cardano-cli turns out to be a pain.
- First I decide to install it like this:
#+begin_example
git clone https://github.com/input-output-hk/cardano-node.git
cd cardano-node/cardano-cli 
cabal update 
cabal build
cabal install
/root/.cabal/bin/cardano-cli --version
#returns: cardano-cli 8.1.0 - linux-x86_64 - ghc-8.10
/root/.cabal/bin/cardano-cli query tip --mainnet
# returns Missing --socket-path SOCKET_PATH
#+end_example
- I have been trying to find the socket path for this node for too long.
- Next options is to specify --socket-path when I start it up or keep trying to resolve this absurd roadblock.
- I do feel a bit abstracted from what I have deployed I am sure there has to be default socket path, I even tried mlocate for
- I noticed two ports bound to 127.0.0.1, running curl 127.0.0.1:12788 I can see there is a web page
- Lets forward it with socat so I can hit that address over tailscale.
#+begin_src tmux :session s1
nix-env -i socat
  socat TCP-LISTEN:5000,reuseaddr,fork TCP:127.0.0.1:12788
#+end_src
- Now I go to my laptop that is logged into tailscale and visit http://100.xx.xx.72:5000 
- YAS I have a very nice dashboard with residency (memory?) allocation rate and productivity. I am not 100% what these relate to but they seem healthy.
- Mmmm I feel like I might want to leave the cardano-cli to someone who understands it better like Jack or Robert.

#current
- We can now deploy a cardano-relay in AWS using, terraform init, terraform user_data (start_node.sh) and configuration.nix
- The node gets started as a service we define in configuration.nix "systemctl status cardano-node-relay-daemon.service"
- The new machine registers itself in tailscale, you can use tailscales to authenticate ssh over the 10. network, you can find machine by ip or networking.hostName
- I can see healty logs with work the node is doing, I can see a dashboard with healty metrics, I still need to query and cli interact with the server
- Next step is cli testing of the node. I think I will ask for help from Jack for this one

*** Troubleshooting tips
- Don't forget to do tcpdump to see where it is trying to get artifacts from

- Things to keep in mind:
#+begin_example
- Still need to figure out how we set configurations for the node ie whitelist block producer etc
- iptables
- network groups in aws
- still need to think about key management
#+end_example



* Next steps. 
** Current
  - Document and test
  - What else is needed? wiregaurd, key management, inventory management, NAT-Gateway, elb(?)
  - Add final parts to make prod ready and move on to block-producer
** Archive of old steps, this is all done

#current
- We can now deploy a cardano-relay in AWS using, terraform init, terraform user_data (start_node.sh) and configuration.nix
- The node gets started as a service we define in configuration.nix "systemctl status cardano-node-relay-daemon.service"
- The machine is also autoconnected to the yumi tailscale network.
- Next step is to document and test what we have.
- Once testing is done we will pivot to terraform/aws structure and account management strategy formalization

  

- Things to keep in mind:
#+begin_example
- Still need to figure out how we set configurations for the node ie whitelist block producer etc
- iptables
- networks in aws, do we allow any internal communication through aws or will everything flow through tailscale?
- still need to think about key management, aegis/sops
#+end_example

* Additional information
- We can start with a copy of https://github.com/bernokl/nix-ops take learnings from https://github.com/yumiai/docs/blob/main/bernoHome/diypool_apply.org and deploy what we can from: input-output-hk/cardano-node
- Step 1, properly review what we did in diypool_apply and compare that to information you can find in iohk/cardano-node to see if we can build up list of steps for deploy
