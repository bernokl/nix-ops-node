* This is document to capture standing up nodes in cardano using divnix and terraform
- This document belongs with https://github.com/bernokl/nix-ops-node
- We are going to explore standing up cardano nodes using out existing divnix and terraform pattern
- The end goal is autonomous deploy of iohk/cardano-node flakes to have relay and then blockproducers.

** Stand up relay node
- Here is the steps I aim to follow:
#+begin_example
- Deploy nixos ec2 instance in aws using terraform
- Clone the cardano-node repository.
- In the cardano-node repository, create a new file called configuration.nix.
- In the configuration.nix file, add the following code:
       {
         imports = [
           "github:input-output-hk/cardano-node?ref=master"
         ];
       }
- Run the following command to build the cardano node:
       nix build github:input-output-hk/cardano-node?ref=master
- Once the cardano node is built, you can start it by running the following command:
       nix run github:input-output-hk/cardano-node?ref=master run
- Here are some additional details about the instructions above:
  - The imports section of the configuration.nix file specifies the Nix flakes that the cardano node depends on.
  - The nix build command builds the cardano node from the Nix flakes that are specified in the imports section.
  - The nix run command starts the cardano node.
  - The cardano-cli tool is used to interact with the cardano node.
#+end_example
- In my repo I am going to copy the terrafor/cache-server to make my relay-node folder.
- I am going to strip the terraform to give me just aws instance:
- I enable envrc with:
#+begin_src tmux :session s1
direnv allow .
#+end_src
- That loads my aws keys into env
- TODO: Ongoing reminder that we need to think about credentials.
- Run init
#+begin_src tmux :session s1
aws_terraform_init
#+end_src
- Apply:
#+begin_src tmux :session s1
aws_terraform_apply
#+end_src
- Grab ip from aws-console ssh in
#+begin_src tmux :session s1
ssh -i id_rsa.pem root@xx.xx.xx.xx
#+end_src
- OK, lets run:
#+begin_src tmux :session s1
nix build github:input-output-hk/cardano-node?ref=master
#+end_src
- Build was less than 3 minuts lets try the run
- Lets pass in run
#+begin_src tmux :session s1
nix run github:input-output-hk/cardano-node?ref=master run
#+end_src
- New error! OO this is why they want you to clone the rope first, lets go look
#+begin_example
InvalidYaml (Just (YamlException "Yaml file not found: configuration/cardano/mainnet-config.json"))

cardano-node: YAML exception:
Yaml file not found: configuration/cardano/mainnet-config.json
#+end_example
- Clone repo to our server
#+begin_src tmux :session s1
git clone https://github.com/input-output-hk/cardano-node.git
#+end_src
- cd
#+begin_src tmux :session s1
  cd cardano-node
#+end_src
- Lets create the configuration.nix
#+begin_example
{
  imports = [
    "github:input-output-hk/cardano-node?ref=master"
  ];
}

#+end_example
- Run from inside repo
#+begin_src tmux :session s1
nix run github:input-output-hk/cardano-node?ref=master run
#+end_src
- Boom, we have a running relay.
- The above should be very easy to add to user_data in terraform.
- Lets strip user_data out to file and:
   - Clone repo
   - Add our configuration.nix
   - Build
   - Run
- Lets update the main.tf to have this:
#+begin_example
 user_data = "${file("start_node.sh")}"
#+end_example
- And lets go crate a start_node.sh
#+begin_example
#!env bash -xe
git clone https://github.com/input-output-hk/cardano-node.git &&
cd cardano-node
cat << 'EOF' > configuration.nix
{
  imports = [
    "github:input-output-hk/cardano-node?ref=master"
  ];
}
EOF
yes | nix build github:input-output-hk/cardano-node?ref=master && 
echo node_done_building > /tmp/outNix
yes | nix run github:input-output-hk/cardano-node?ref=master run
#+end_example
- Now we destroy the host and start the apply again so that start_node.sh can run.
- We can manually build and start a cardano relay by running: aws_terraform_apply
- Right now the node is up, and I see a  process: nix build github:input-output-hk/cardano-node?ref=master
- If I strace there is activity, also we are steadily using more disk space.
- I do not understand why my manual build was so much quicker. 
- It has been building for almost exactly 2 hours. I do see the load is 12 on 4 cores meaning the cpu is not nearly keeping up. 
- I think I might have scaled to 2xlarge or even 4xlarge for the build phase in diypool, might have to consider doing the same here.
- Will leave it to run for now, I wish I had a sense of % done
- It took a couple of hours of google and play, but came up with:
#+begin_src tmux :session s1
nix build --accept-flake-config github:input-output-hk/cardano-node?ref=master
#+end_src
- I got it to work, but still have some unexpected behaviuor.
- Let me destroy and rebuild with everything vanilla then try to run nix build --accept-flake-config on clean os
- YAS, running this manually builds a node I can run in 5 minutes
#+begin_src tmux :session s1
nix build --accept-flake-config github:input-output-hk/cardano-node?ref=master
#+end_src
- OK going to destroy and build with that in my startup_node.sh
#+begin_example
nix build --accept-flake-config github:input-output-hk/cardano-node?ref=master &&
echo we_got_clean_build > /tmp/outNix
nix run --accept-flake-config github:input-output-hk/cardano-node?ref=master run
#+end_example
- This builds the node! and I think starts it, BUT it does not say running.
- The matrue solution would be to have the run executed by a daemon service like system.d.
- At this point I fell into a multi hour investigation into adding traditional /etc/systemd/system/*.service I could run.
- Turns out nix wants you to define your service in configurattion.nix
- It seems like you add it to configurion something like:
#+begin_example
config.systemd.services.interosEsMdb = {
  description = "Interos MongoDB+ES log capture";
  after = ["network.target"];
  wantedBy = ["multi-user.target"];

  serviceConfig = {
    # change this to refer to your actual derivation
    ExecStart = "${interosEsMdb}/bin/syslog-exec.sh";
    EnvironmentFile = "${interosEsMdb}/lib/es-service.env";
    Restart = "always";
    RestartSec = 1;
  }
#+end_example
- Lots of itteration later I ended with this:
#+begin_example
  systemd.services.cardano-node-relay-daemon = {
    enable = true;
    description = "Cardano relay daemon";
    after = ["network.target"];
    wantedBy = ["multi-user.target"];

    serviceConfig = {
      ExecStart = "${pkgs.nix}/bin/nix run --accept-flake-config github:input-output-hk/cardano-node?ref=master run";
      Restart = "always";
      User = "root";
      WorkingDirectory="/cardano-node/";
      RestartSec = 1;
    };
  };
#+end_example
- I also needed to add line to startup_node.sh to start the service
#+begin_example
systemctl start cardano-node-relay-daemon.service
#+end_example

- Nice cheat to find aws ec2 external ip, replace running with tag or other metadata you car about
#+begin_src tmux :session s1
aws ec2 describe-instances --filters 'Name=instance-state-name,Values=running' --query 'Reservations[*].Instances[*].[InstanceId,PublicIpAddress]' --output text
#+end_src
- returns:
#+begin_example
i-01a7e8d4e89049894     13.239.136.44
#+end_example
- And on this host I see the daemon running:
#+begin_example
> systemctl status cardano-node-relay-daemon.service 
● cardano-node-relay-daemon.service - Cardano relay daemon
     Loaded: loaded (]8;;file://ip-172-31-19-21.ap-southeast-2.compute.internal/etc/systemd/system/cardano-node-relay-daemon.service/etc/systemd/system/cardano-node-relay-daemon.service]8;;; enabled; preset: enabled)]8;;
     Active: active (running) since Mon 2023-05-08 13:47:51 UTC; 4h 24min ago
   Main PID: 2101 (cardano-node)
         IP: 15.2G in, 157.6M out
         IO: 316.0K read, 18.3G written
      Tasks: 16 (limit: 9155)
     Memory: 6.1G
        CPU: 8h 50min 11.556s
     CGroup: /system.slice/cardano-node-relay-daemon.service
             └─2101 /nix/store/0ndig34c9qizj3g4z1s1scwk3pxcvfzn-cardano-node-exe-cardano-node-8.0.0/bin/cardano-node>

May 08 18:12:16 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:18 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:19 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:20 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:21 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:23 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:24 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:25 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:26 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
May 08 18:12:28 ip-172-31-19-21.ap-southeast-2.compute.internal nix[2101]: [ip-172-3:cardano.node.ChainDB:Notice:35]>
#+end_example
- Robert suggeted I focus on tailscale integration next as he already had the template on hokioi
- Here is the updates I made to get the ec2 instance running tailscale and connecting to yumi network
#+begin_example
 { config, lib, pkgs, modulesPath, ... }:
+let
+   system.autoUpgrade.channel = "https://nixos.org/channels/nixos-unstable";
+   nixos-unstable = import <nixos-unstable> {};
 
-{
+in {
   imports = [ "${modulesPath}/virtualisation/amazon-image.nix" ];
 
   ec2.hvm = true;
@@ -26,10 +29,47 @@
     };
   };
 
+ services.tailscale.enable = true;
+
+ systemd.services.tailscale-autoconnect = {
+    description = "Automatic connection to Tailscale";
+
+    # make sure tailscale is running before trying to connect to tailscale
+    after = [ "network-pre.target" "tailscale.service" ];
+    wants = [ "network-pre.target" "tailscale.service" ];
+    wantedBy = [ "multi-user.target" ];
+
+    # set this service as a oneshot job
+    serviceConfig.Type = "oneshot";
+
+    # have the job run this shell script
+    script = with pkgs; ''
+      # wait for tailscaled to settle
+      sleep 2
+
+      # check if we are already authenticated to tailscale
+      status="$(${tailscale}/bin/tailscale status -json | ${jq}/bin/jq -r .BackendState)"
+      if [ $status = "Running" ]; then # if so, then do nothing
+        exit 0
+      fi
+
+      # otherwise authenticate with tailscale
#
+      ${tailscale}/bin/tailscale up --ssh -authkey tskey-auth-########
+    '';
+};
+
+  networking.firewall = {
+    checkReversePath = "loose";
+    enable = true;
+    trustedInterfaces = [ "tailscale0" ];
+    allowedUDPPorts = [ config.services.tailscale.port ];
+  };
+
+  networking.hostName = "aws-1";
+  networking.domain = "husky-ostrich.ts.net";

   environment.systemPackages = with pkgs; [
     git
     vim
     htop
+    tailscale
     lsof
   ];
 }

#+end_example
- Note we give the machine a networking.hostName, that registers the name we want for this machine in tailscale
- Also VERY important once it is connected to tailscale your ssh sessions over the 10. network with be authenticated through tailscale.
- This is a very important bennefit.
- Also very important, the authkey used needs to be set to be ephemiral, pre-authenticate the hosts and assign tags we want for the machines.
- This makes management very simple, but needs to be carefully managed.
- We will integrate SOPS/1Password/Key-store to hold keys we can then hydrate on host with env-vars in our session.
- Trying to do some testing, lets start with what we can see on the node:
#+begin_example
journalctl -u cardano-node-relay-daemon.service

May 10 18:08:04 aws-1 nix[2100]: Event: LedgerUpdate (HardForkUpdateInEra S (S (Z (WrapLedgerUpdate {unwrapLedgerUpdate = ShelleyUpdatedProtocolUpdates []}))))
May 10 18:08:04 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:04.96 UTC] Chain extended, new tip: 2198c40091993baed54b4638473327b0b77c5dccaa56768690f5b56>
May 10 18:08:06 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:06.21 UTC] Chain extended, new tip: 2e7ccf635d45201aaf52c5a2e7e10f7c5b90a2ca5ed10356210859e>
May 10 18:08:07 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:07.46 UTC] Chain extended, new tip: 18ba572b54363a6bcb43bccb283828ab559cd5ddf7d73c2b0c07c51>
May 10 18:08:08 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:08.71 UTC] Chain extended, new tip: 87cfa4a2f2258217adbde872e2ab53906f43d95e1f9fbbbc4dc362a>
May 10 18:08:09 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:09.53 UTC] Chain extended, new tip: 8add136563b0c36616f42ad52815e3c666be1b8b087e4268949395c>
May 10 18:08:09 aws-1 nix[2100]: Event: LedgerUpdate (HardForkUpdateInEra S (S (Z (WrapLedgerUpdate {unwrapLedgerUpdate = ShelleyUpdatedProtocolUpdates [ProtocolUpdate {protocolUpda>
May 10 18:08:09 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:09.54 UTC] Chain extended, new tip: e6878f21c35b5c9c233bf54207c28dbeb0743c6cf19d1468afc78c6>
May 10 18:08:10 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:10.79 UTC] Chain extended, new tip: 92fc5c7b7e6b8a84623787b2b3a52400d9388958258e7cdf57c9d6f>
May 10 18:08:12 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:12.04 UTC] Chain extended, new tip: f02680481aa08d0d53b4d1574d063fcba04d8b10b8e91ef07c52737>
May 10 18:08:13 aws-1 nix[2100]: [aws-1:cardano.node.ChainDB:Notice:35] [2023-05-10 18:08:13.29 UTC] Chain extended, new tip: 83fdc9ce9b078a25880964ab7c87bf5e66a73849fbe06918580f738>
#+end_example
- That seems positive confirmation that we are participating in the network traffic.
- Going to try to query the node using cardano-cli turns out to be a pain.
- First I decide to install it like this:
#+begin_example
git clone https://github.com/input-output-hk/cardano-node.git
cd cardano-node/cardano-cli 
cabal update 
cabal build
cabal install
/root/.cabal/bin/cardano-cli --version
#returns: cardano-cli 8.1.0 - linux-x86_64 - ghc-8.10
/root/.cabal/bin/cardano-cli query tip --mainnet
# returns Missing --socket-path SOCKET_PATH
#+end_example
- I have been trying to find the socket path for this node for too long.
- Next options is to specify --socket-path when I start it up or keep trying to resolve this absurd roadblock.
- I do feel a bit abstracted from what I have deployed I am sure there has to be default socket path, I even tried mlocate for
- I noticed two ports bound to 127.0.0.1, running curl 127.0.0.1:12788 I can see there is a web page
- Lets forward it with socat so I can hit that address over tailscale.
#+begin_src tmux :session s1
nix-env -i socat
  socat TCP-LISTEN:5000,reuseaddr,fork TCP:127.0.0.1:12788
#+end_src
- Now I go to my laptop that is logged into tailscale and visit http://100.xx.xx.72:5000 
- YAS I have a very nice dashboard with residency (memory?) allocation rate and productivity. I am not 100% what these relate to but they seem healthy.
- Mmmm I feel like I might want to leave the cardano-cli to someone who understands it better like Jack or Robert.

  
#current
- We can now deploy a cardano-relay in AWS using, terraform init, terraform user_data (start_node.sh) and configuration.nix
- The node gets started as a service we define in configuration.nix "systemctl status cardano-node-relay-daemon.service"
- The new machine registers itself in tailscale, you can use tailscales to authenticate ssh over the 10. network, you can find machine by ip or networking.hostName
- I can see healty logs with work the node is doing, I can see a dashboard with healty metrics, I still need to query and cli interact with the server
- Next step is cli testing of the node. I think I will ask for help from Jack for this one

** Update terraform structure of nix-ops-node to implement terragrunt.
- First lets add some folders: accounts/sandbox/ap-southeast2
- Now we move node-relay into that region ie accounts/sandbox/ap-southeast2/node-relay-1
- The 5 files in that directory (main.tf, terragrunt.hcl, configuration.nix, start_node.sh, variables.tf) will allow you to spin up a machine
- Here is our layout:
#+begin_example

terraform
├── modules
│   ├ node-relay (goal would be to set common features here)
│   ├── main.tf
│   ├── variables.tf
├── accounts
│   ├ sandbox
│   ├── terragrunt.hcl(This sets up our .tfstate in s3, long term this would keep common configs like, machine type or other sandbox components like security_groups?)
│   ├── ap-southeast-2
│   ├────── terragrunt.hcl(I did not keep, but do we have common components that would be here?) 
│   ├────── node-relay-1 (This works as is in the repo)
│   ├───────  main.cf (This is self contained and works without the module)
│   ├───────  configuration.nix (This contains initial machine state including tailscale and setting up iohk/cardano-node service)
│   ├───────  start_node.sh ( This imports cardano-node referer, builds the flake and then starts the service we set up in configuration.nix)
│   ├───────  terragrunt.hcl (This passes in variables, for this POC it just passes in machine type, but can be expanded.
│   ├───────  variables.tf (Defines the variables used by the module)
│   ├────── node-relay-2 (This is experiment to be more DRY, it would mean less duplication of code in main.tf by re-using what we have in modules)
│   ├───────  main.cf (Sources modules/node-relay and passes in variblees it needs.)
│   ├───────  configuration.nix (Same as above)
│   ├───────  start_node.sh (Same as above)
│   ├───────  terragrunt.hcl (sources modules/node-relay *note the source in main.tf should not be needed, but this is still in testing, provides values to varibles.
│   ├───────  variables.tf (Defines the variables used by the module)
│   ├── ap-southeast-2
│   ├────── terragrunt.hcl(I did not keep, but do we have common components that would be here?) 
│   ├────── node-relay-1 (This works as is in the repo)
│   ├───────  main.cf (This is self contained and works without the module)
│   ├───────  configuration.nix (This contains initial machine state including tailscale and setting up iohk/cardano-node service)
│   ├───────  start_node.sh ( This imports cardano-node referer, builds the flake and then starts the service we set up in configuration.nix)
│   ├───────  terragrunt.hcl (This passes in variables, for this POC it just passes in machine type, but can be expanded.
│   ├───────  variables.tf (Defines the variables used by the module)

#+end_example
- Before you can deploy anything in the repo you will need to replace the tailscale key in configuration.nix and the whitelist ip address to one that will ssh in. 
- TODO: Decide if we want to allow ssh outside tailscale perhaps not? The applied machine is accessible from tailscale, we can alwaays manually add whiteliting if we can not get to it from tailscale
- Apply your changes with:
#+begin_src tmux :session s1
terragrunt init &&
terragrunt apply
#+end_src
- node-relay-1 works as expected we still manually add TS-key, whitelist ip and end up with SSH key locally.
- TODO: incorporate sops to handle keys and secrets
- node-relay-2 is WORKING! (can you tell it was a pain?)
- node-relay-2 is more DRY, main.tf is only call out to the module. 
- Next I am going to create copies of relay-1 in 2 other regions provide each with unique name and key see how we can interact between them.
- Our 3 regions for this proof of concept (can always change in new regions)
#+begin_example
ap-southeast-2 (Asia Pacific Sydney)
eu-north-1 (Europe Stockholm) 
ap-south-1 (Asia Pacific Mumbai)
#+end_example
- Here are the steps to create a new region:
- Copy a known good directory - I am copying sanbox/ap-southeast-2 naming the copy ap-south-1
- I am going to re-use only structure of node-relay-1 because I only need one node in each region, and architectually I am not sold on making nodes into a module, I want this to be easilly replicated to other infrastructures. I am wondering if the directory structure is too complex. I am keeping it for ease of remote state management, but I am open to improvement suggestions.
- main.tf:
#+begin_example
- You will need to update the provider.region to your new region
- Make sure the nix_image.source has reference to a commit that has an image specified for the region you are setting up
- Make sure any ip's you need whitelisted is in aws_security_group.ssh_and_egress.from_port=22 cidr_blocks, 
- TODO if it is a relay-node we should be able to reach over tailscale then perhaps we want to get rid of port 22 whitelisting in aws_security_group
- For aws_instance.machine.subnet_id you will currently need to go look this up in aws-console-vpc-subnets
#+end_example
- For the above we have a dependency on keeping up the images list in a fork of https://github.com/nix-community/terraform-nixos.git we host.
- TODO: There should be automation around https://github.com/nix-community/terraform-nixos/blob/master/aws_image_nixos/url_map.tf it is currently a manual update of the copy our code points to.
- configuration.nix:
#+begin_example
- Make sure you are using the right tskey-auth-xxx 
- Update the networking.hostName 
#+end_example
- We will add a .envrc to this folder to update our AWS_REGION for ap-south1 I did:
#+begin_example
# I source the original envrc that gets unloaded when I cd in here
source ../../../../.envrc
# I add the region we will be using
export AWS_REGION=ap-south-1
#+end_example
- This works to deploy a new node to ap-south-1, lets do the same for eu-north-1
- Made copy of ap-south1 to create eu-north1
- Made the same updates to main.tf, configuration.nix and .envrc
- I set my region in aws by running "direnv allow ." from the eu-north1/
- I confirm I am pointing to the right region with
#+begin_src tmux :session s1
aws configure get region
#+end_src
- Next I run terragrunt init/apply
- And I see my new aws-eu-n-1-1 node in tailscale, and YUP it has a running node.

 

*** Troubleshooting tips
- Don't forget to do tcpdump to see where it is trying to get artifacts from

- Things to keep in mind:
#+begin_example
- Still need to figure out how we set configurations for the node ie whitelist block producer etc
- iptables
- network groups in aws
- still need to think about key management
#+end_example



* Next steps. 
** Current
  - Document and test
  - What else is needed? wiregaurd, key management, inventory management, NAT-Gateway, elb(?)
  - Add final parts to make prod ready and move on to block-producer
** Archive of old steps, this is all done

#current
- We can now deploy a cardano-relay in AWS using, terraform init, terraform user_data (start_node.sh) and configuration.nix
- The node gets started as a service we define in configuration.nix "systemctl status cardano-node-relay-daemon.service"
- The machine is also autoconnected to the yumi tailscale network.
- Next step is to document and test what we have.
- Once testing is done we will pivot to terraform/aws structure and account management strategy formalization

  

- Things to keep in mind:
#+begin_example
- Still need to figure out how we set configurations for the node ie whitelist block producer etc
- iptables
- networks in aws, do we allow any internal communication through aws or will everything flow through tailscale?
- still need to think about key management, aegis/sops
#+end_example

* Additional information
- We can start with a copy of https://github.com/bernokl/nix-ops take learnings from https://github.com/yumiai/docs/blob/main/bernoHome/diypool_apply.org and deploy what we can from: input-output-hk/cardano-node
- Step 1, properly review what we did in diypool_apply and compare that to information you can find in iohk/cardano-node to see if we can build up list of steps for deploy
